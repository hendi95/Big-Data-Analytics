{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neAV6tdFTY3U"
      },
      "source": [
        "# Big Data Analytics Homework 02\n",
        "\n",
        "*Complete this assignment in Google Colab. Prior to submitting a copy of this notebook (.ipynb format), run every cell and ensure you have corrected all runtime errors. Be sure to fill in your Name and SUID in the following cell. As always, you must do your own work. This means you may not use answers to the following questions generated by any other person or a generative AI tool such as ChatGPT. You may, however, discuss this assignment with others in a general way and seek help when you need it, but, again, you must do your own work.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i9feybmTkum"
      },
      "source": [
        "Name:\n",
        "\n",
        "SUID:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMhlq-j15puN"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uUjg3KNnWWqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20551e4-f6cd-461c-8092-0b83a8ad7a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M4RbOhX6Wbaz"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import functions as fn\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "spark = SparkSession\\\n",
        "    .builder\\\n",
        "    .appName('Homework 02')\\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifbPI0MyTCnk"
      },
      "source": [
        "## RDDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5vi8jghS_vJ"
      },
      "source": [
        "### Q1\n",
        "\n",
        "Create a single-dimensional PySpark RDD named `bernoulli_rdd` that contains 1,000 Bernoulli probability distribution data points consisting of integers 0 or 1 with P(0) = P(1) = 0.5.\n",
        "\n",
        "Use only PySpark RDDs and `map` to complete this question.\n",
        "\n",
        "Hint: Use the [`RandomRDDs.uniformRDD`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.mllib.random.RandomRDDs.html#pyspark.mllib.random.RandomRDDs.uniformRDD) function for your sampling step. But you will still need to map over this RDD with a custom function that creates 1's and 0's from the initial output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Odr2DBddYMI5"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "from pyspark.mllib.random import RandomRDDs\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"BernoulliRDD\").getOrCreate()\n",
        "\n",
        "# Use the RandomRDDs.uniformRDD function to create random uniform values\n",
        "random_uniform_values = RandomRDDs.uniformRDD(spark, size=1000)\n",
        "\n",
        "# Define a function to map uniform values to Bernoulli values (0 or 1)\n",
        "def map_to_bernoulli(value):\n",
        "    if value < 0.5:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# Apply the mapping function to the uniform values RDD\n",
        "bernoulli_rdd = random_uniform_values.map(map_to_bernoulli)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vvFGIsSfbGPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9bc6aa1-f9e5-41db-afbd-e701b7d06a81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# do not modify\n",
        "bernoulli_rdd.take(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ljPLc8MYHFq"
      },
      "source": [
        "### Q2\n",
        "\n",
        "Using only Spark, count and display the number of 1's and the number of 0's in `bernoulli_rdd`.\n",
        "\n",
        "Your output must be of the form \"There are X 1's and Y 0's in bernoulli_rdd\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ey-ybD9_9oxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735931b3-ac1e-406d-8a1b-12cc11137ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 475 1's and 525 0's in bernoull_rdd.\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# Count the number of 1's and 0's in bernoulli_rdd\n",
        "count_ones = bernoulli_rdd.filter(lambda x: x == 1).count()\n",
        "count_zeros = bernoulli_rdd.filter(lambda x: x == 0).count()\n",
        "\n",
        "# Display the results\n",
        "print(f\"There are {count_ones} 1's and {count_zeros} 0's in bernoull_rdd.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYmrTabq49RG"
      },
      "source": [
        "### Q3\n",
        "\n",
        "Create a two new 2-dimensional RDD named `bernoulli_sample_rdd_1` and `bernoulli_sample_rdd_2` that each contain sample data from `bernoulli_rdd`. Each element of these RDDs should contain 10 samples (with replacement) from the original 1,000. The length of each RDD should be the number of samples which should be 50. In addition to the samples themselves, each data element in each RDD should contain `sample_number`, which should be calculated from each sample, not \"hard-coded\" as 10.\n",
        "\n",
        "A sample element of the result will be of the form `[(sample_number, [sample])]`.\n",
        "\n",
        "`bernoulli_sample_rdd_1` should be created using the [`sample`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sample.html) method and `bernoulli_sample_rdd_2` should be created using the[`takeSample`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.takeSample.html) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7JZvacaF6ENQ"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "# Create bernoulli_sample_rdd_1 using the sample method\n",
        "sample_size = 10\n",
        "num_samples = 50\n",
        "\n",
        "bernoulli_sample_rdd_1 = bernoulli_rdd.sample(withReplacement=True, fraction=sample_size / bernoulli_rdd.count())\n",
        "# Add the sample_number to each element\n",
        "bernoulli_sample_rdd_1 = bernoulli_sample_rdd_1.map(lambda sample: (sample, [sample_number for sample_number in range(num_samples)]))\n",
        "\n",
        "# Create bernoulli_sample_rdd_2 using the takeSample method\n",
        "bernoulli_sample_rdd_2 = bernoulli_rdd.takeSample(True, int(sample_size * bernoulli_rdd.count()))\n",
        "\n",
        "# Add the sample_number to each element\n",
        "bernoulli_sample_rdd_2 = sc.parallelize(bernoulli_sample_rdd_2).map(lambda sample: (sample, [sample_number for sample_number in range(num_samples)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CdYQv3prciZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d2ecc2d-ee55-443c-e2f0-1d022bbcd93d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (1,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (1,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49])]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# do not modify\n",
        "bernoulli_sample_rdd_1.take(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ymlvde4wEIss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8bdb13e-9e69-4612-be75-c2ba3f0023e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (1,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49]),\n",
              " (0,\n",
              "  [0,\n",
              "   1,\n",
              "   2,\n",
              "   3,\n",
              "   4,\n",
              "   5,\n",
              "   6,\n",
              "   7,\n",
              "   8,\n",
              "   9,\n",
              "   10,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   14,\n",
              "   15,\n",
              "   16,\n",
              "   17,\n",
              "   18,\n",
              "   19,\n",
              "   20,\n",
              "   21,\n",
              "   22,\n",
              "   23,\n",
              "   24,\n",
              "   25,\n",
              "   26,\n",
              "   27,\n",
              "   28,\n",
              "   29,\n",
              "   30,\n",
              "   31,\n",
              "   32,\n",
              "   33,\n",
              "   34,\n",
              "   35,\n",
              "   36,\n",
              "   37,\n",
              "   38,\n",
              "   39,\n",
              "   40,\n",
              "   41,\n",
              "   42,\n",
              "   43,\n",
              "   44,\n",
              "   45,\n",
              "   46,\n",
              "   47,\n",
              "   48,\n",
              "   49])]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# do not modify\n",
        "bernoulli_sample_rdd_2.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62UeDcsJHzRa"
      },
      "source": [
        "### Q4\n",
        "\n",
        "Explain key difference between `bernoulli_sample_rdd_1` and `bernoulli_sample_rdd_2` and the reason for it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TSg9k8q6RYe"
      },
      "source": [
        "*answer here*\n",
        "The key difference between bernoulli_sample_rdd_1 and bernoulli_sample_rdd_2 lies in the method used to create these RDDs, which is the sample method for bernoulli_sample_rdd_1 and the takeSample method for bernoulli_sample_rdd_2.\n",
        "\n",
        "**bernoulli_sample_rdd_1 - sample method - withReplacement = True**\n",
        "In this case, we created bernoulli_sample_rdd_1 by using the sample technique with replacement, which enables the use of the identical elements from the original RDD more than once in the sampled RDD. This explains why distinct sets of 10 samples each have the same sample numbers (e.g. 1) associated with them. The bernoulli_sample_rdd_1 dataset contains one duplicate sample for every sample that was randomly chosen with replacement from the original RDD.\n",
        "\n",
        "**bernoulli_sample_rdd_2 - takeSample method - withReplacement = True**\n",
        "bernoulli_sample_rdd_2 is produced by takeSample. Unlike the sample method, which returns an RDD, this method takes a random sample of a defined size from the original RDD with or without replacement. The list of sampled items that is returned, however, does not contain duplicates because the same element cannot be included more than once.\n",
        "In this instance, we produced bernoulli_sample_rdd_2 by using the takeSample method with replacement. There are various sample numbers associated with each sets of 10 samples because it directly returns a list and each element in the original RDD can only be selected once or with replacement (if withReplacement is set to True). In bernoulli_sample_rdd_2, each element is a distinct sample taken from the original RDD.\n",
        "\n",
        "Another difference between these two RDDs is in how the sample size is determined: bernoulli_sample_rdd_1 uses a fraction of the total, while bernoulli_sample_rdd_2 uses a fixed number of elements for sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxY8j-KsruTt"
      },
      "source": [
        "### Q5\n",
        "\n",
        "Re-using code from above that created `bernoulli_sample_rdd_2`, create `bernoulli_sample_rdd_3` which has 100 observations per sample.\n",
        "\n",
        "Using PySpark `map`, create a new RDD named `bernoulli_sample_mean_rdd` that contains the sampling distribution of the means of the samples contained in `bernoulli_sample_rdd_3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hMV3qgoBIrrU"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "# Create bernoulli_sample_rdd_3 using the takeSample method with 100 observations per sample\n",
        "sample_size = 100\n",
        "bernoulli_sample_rdd_3 = bernoulli_rdd.takeSample(True, sample_size * num_samples)\n",
        "\n",
        "# Add the sample_number to each element\n",
        "bernoulli_sample_rdd_3 = sc.parallelize(bernoulli_sample_rdd_3).map(lambda sample: (sample, [sample_number for sample_number in range(num_samples)]))\n",
        "\n",
        "# Calculate the mean of each sample and create bernoulli_sample_mean_rdd\n",
        "bernoulli_sample_mean_rdd = bernoulli_sample_rdd_3.map(lambda sample: (sample[0], sum(sample[1]) / num_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JPiA2MM7JiMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fdea493-2dd2-42c6-c2be-fccd2ba06d13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 24.5),\n",
              " (0, 24.5),\n",
              " (1, 24.5),\n",
              " (0, 24.5),\n",
              " (0, 24.5),\n",
              " (1, 24.5),\n",
              " (0, 24.5),\n",
              " (1, 24.5),\n",
              " (0, 24.5),\n",
              " (1, 24.5)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# do not modify\n",
        "bernoulli_sample_mean_rdd.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt-MYZymelbp"
      },
      "source": [
        "## DataFrames\n",
        "\n",
        "In this section we will work with data from the U.S. Environmental Protection Agency (EPA). There are two data sets. The first data set consists of daily **temperatures** collected at the U.S. **city** level. The second data set consists of daily **air quality** data collected at the U.S. **county** level. These measurements were taken for the full year 2021."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bOzs4146LDwb"
      },
      "outputs": [],
      "source": [
        "# download the temperature and aqi data sets\n",
        "%%bash\n",
        "\n",
        "if [[ ! -f us-daily-temperatures-2021.csv.csv ]]; then\n",
        " wget https://syr-bda.s3.us-east-2.amazonaws.com/us-daily-temperatures-2021.csv -q\n",
        "fi\n",
        "\n",
        "if [[ ! -f us-daily-aqi-2021.csv.csv ]]; then\n",
        " wget https://syr-bda.s3.us-east-2.amazonaws.com/us-daily-aqi-2021.csv -q\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwX-8dwvCytd"
      },
      "source": [
        "### Q6\n",
        "\n",
        "Load the temperature data (using Spark) into a data frame called `temperature`. Load the air quality data into a data frame called `aqi`. Print the schema and the number of rows for each data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HgZVlQZQCee-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9474b4e-d46c-45ca-9de8-d9401d99d875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature Data Schema:\n",
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- county: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- sites_reporting: integer (nullable = true)\n",
            " |-- mean_temperature_f: double (nullable = true)\n",
            " |-- max_temp_f: double (nullable = true)\n",
            " |-- max_temp_hour: integer (nullable = true)\n",
            "\n",
            "Number of Rows in Temperature Data:  206292\n",
            "Air Quality Data Schema:\n",
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- county: string (nullable = true)\n",
            " |-- aqi: integer (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            "\n",
            "Number of Rows in Air Quality Data:  130922\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# Load temperature data into a DataFrame\n",
        "temperature = spark.read.csv(\"us-daily-temperatures-2021.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Load air quality data into a DataFrame\n",
        "aqi = spark.read.csv(\"us-daily-aqi-2021.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Print the schema and row counts for each DataFrame\n",
        "print(\"Temperature Data Schema:\")\n",
        "temperature.printSchema()\n",
        "print(\"Number of Rows in Temperature Data: \", temperature.count())\n",
        "\n",
        "print(\"Air Quality Data Schema:\")\n",
        "aqi.printSchema()\n",
        "print(\"Number of Rows in Air Quality Data: \", aqi.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpdnW2t0JGkW"
      },
      "source": [
        "The `temperature` data is reported at the city level, but the `aqi` data is at the county level. We want the \"grain\" of these data sets to match. This means we need to aggregate the the `temperature` data to the county level. This is tricky because we could possibly have counties in different states with the same name. This means you'll want to aggregate not just at the county level (by date), but at the state *and* county level (by date). We also have 4 different metrics to deal with: the total number of sites, a mean, a max, and one value — `max_temp_hour` — that corresponds to the value of another metric, `max_temp_f`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEYrM7vlG16v"
      },
      "source": [
        "### Q7\n",
        "\n",
        "Create a new data frame called `temperature_county` that contains the **mean** temperature, the **max** temperature, and the **total** sites reporting, for each unique `date`, `state`, and `county` in the `temperature` data frame. The columns `mean_temperature_f`, `max_temp_f`, and `sites_reporting` should retain their names. Additionally, round the **mean** and **max** columns to the nearest whole number and cast them as integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "13UaiI2sJ7M8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e05684-2566-4bb6-bf87-65360ad81646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+--------------+------------------+----------+---------------+\n",
            "|      date|         state|        county|mean_temperature_f|max_temp_f|sites_reporting|\n",
            "+----------+--------------+--------------+------------------+----------+---------------+\n",
            "|2021-01-01|       Montana|        Fergus|                34|        43|              1|\n",
            "|2021-01-01|         Texas|         Titus|                40|        46|              1|\n",
            "|2021-01-02|          Iowa|       Johnson|                18|        22|              1|\n",
            "|2021-01-02|      Oklahoma|      Garfield|                34|        42|              1|\n",
            "|2021-01-02|  South Dakota|        Custer|                37|        42|              1|\n",
            "|2021-01-03| West Virginia|         Mason|                41|        44|              1|\n",
            "|2021-01-04|North Carolina|       Forsyth|                43|        50|              1|\n",
            "|2021-01-04|         Texas|          Rusk|                54|        67|              1|\n",
            "|2021-01-04|          Utah|          Utah|                31|        48|              2|\n",
            "|2021-01-04|     Wisconsin|          Dane|                24|        30|              1|\n",
            "|2021-01-05| Massachusetts|     Worcester|                31|        34|              2|\n",
            "|2021-01-05|      Missouri|St. Louis City|                40|        53|              1|\n",
            "|2021-01-06|      Colorado|          Mesa|                26|        40|              2|\n",
            "|2021-01-06|North Carolina|       Forsyth|                40|        48|              1|\n",
            "|2021-01-06|  South Dakota|        Custer|                34|        43|              1|\n",
            "|2021-01-06|          Utah|          Utah|                30|        42|              2|\n",
            "|2021-01-07|    California|        Madera|                43|        51|              1|\n",
            "|2021-01-07|      Maryland|    Washington|                36|        41|              1|\n",
            "|2021-01-07|      Michigan|        Ingham|                33|        36|              1|\n",
            "|2021-01-07|  North Dakota|          Dunn|                29|        35|              1|\n",
            "+----------+--------------+--------------+------------------+----------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# your code\n",
        "from pyspark.sql.functions import round, mean, max, count\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Group the \"temperature\" DataFrame by the columns \"date,\" \"state,\" and \"county\"\n",
        "temperature_county = temperature.groupBy(\"date\", \"state\", \"county\") \\\n",
        "    .agg(\n",
        "        # Round the mean of \"mean_temperature_f\" to the nearest whole number\n",
        "        round(mean(\"mean_temperature_f\")).cast(IntegerType()).alias(\"mean_temperature_f\"),\n",
        "        # Round the maximum value of \"max_temp_f\" to the nearest whole number\n",
        "         round(max(\"max_temp_f\")).cast(IntegerType()).alias(\"max_temp_f\"),\n",
        "        # Count the number of records in each group and create a column \"sites_reporting\"\n",
        "         count(\"sites_reporting\").alias(\"sites_reporting\"))\n",
        "\n",
        "temperature_county.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "prkZa3rNJ8cH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eed7ca4-e917-4c33-8d4c-6137d4bc7b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows in temperature_county: 138555\n",
            "+----------+--------+--------------------+------------------+----------+---------------+\n",
            "|      date|   state|              county|mean_temperature_f|max_temp_f|sites_reporting|\n",
            "+----------+--------+--------------------+------------------+----------+---------------+\n",
            "|2021-01-01| Alabama|            Escambia|                64|        69|              1|\n",
            "|2021-01-01| Alabama|           Jefferson|                66|        74|              1|\n",
            "|2021-01-01|  Alaska|              Denali|                -1|         6|              1|\n",
            "|2021-01-01|  Alaska|Fairbanks North Star|                -9|        -1|              2|\n",
            "|2021-01-01| Arizona|             Cochise|                40|        49|              1|\n",
            "|2021-01-01| Arizona|            Coconino|                29|        35|              1|\n",
            "|2021-01-01| Arizona|            Maricopa|                49|        65|              1|\n",
            "|2021-01-01| Arizona|              Navajo|                30|        42|              1|\n",
            "|2021-01-01| Arizona|                Pima|                47|        63|              4|\n",
            "|2021-01-01|Arkansas|             Pulaski|                42|        45|              1|\n",
            "+----------+--------+--------------------+------------------+----------+---------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not modify\n",
        "print('Rows in temperature_county:', temperature_county.count())\n",
        "temperature_county.orderBy('date', 'state', 'county').show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLsQFW_yRh8I"
      },
      "source": [
        "### Q8\n",
        "\n",
        "Create a new data frame called `county_max_temp_hour` that reports the `max_temp_hour` at the same level of aggregation as `temperature_county` in the previous step. This means it should have the **same number of rows** as `temperature_county` and contain the same grouping fields, but only one metric, `max_temp_hour`. Once you have created this data frame, **left join** it to `temperature county` as a new data frame called `temperature_county_final`.\n",
        "\n",
        "I've provided some starter code for you below. Fill in where you see `???` in order to complete the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FOO26LZBWNnk"
      },
      "outputs": [],
      "source": [
        "# your code\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, desc\n",
        "\n",
        "# Define a window spec\n",
        "window_spec = Window.partitionBy(\"date\", \"state\", \"county\")\\\n",
        "  .orderBy(desc(\"max_temp_f\"))\n",
        "\n",
        "# Add row number for each group\n",
        "df = temperature\\\n",
        "  .withColumn('row_num', row_number().over(window_spec))\n",
        "\n",
        "# Filter to the first row using row_num\n",
        "df2 = df.where(df[\"row_num\"] == 1)\n",
        "\n",
        "# Keep only the grouping columns and max_temp_hour\n",
        "county_max_temp_hour = df2.select(\"date\", \"state\", \"county\", \"max_temp_hour\")\n",
        "\n",
        "# Left join county_max_temp_hour to temperature_county\n",
        "temperature_county_final = temperature_county.join(county_max_temp_hour, [\"date\", \"state\", \"county\"], \"left\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9ngrLyD3WRQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6117aa-3b64-4492-9610-999a5547d186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows in county_max_temp_hour: 138555\n",
            "+----------+--------+--------------------+-------------+\n",
            "|      date|   state|              county|max_temp_hour|\n",
            "+----------+--------+--------------------+-------------+\n",
            "|2021-01-01| Alabama|            Escambia|            0|\n",
            "|2021-01-01| Alabama|           Jefferson|           13|\n",
            "|2021-01-01|  Alaska|              Denali|            4|\n",
            "|2021-01-01|  Alaska|Fairbanks North Star|           12|\n",
            "|2021-01-01| Arizona|             Cochise|           12|\n",
            "|2021-01-01| Arizona|            Coconino|           12|\n",
            "|2021-01-01| Arizona|            Maricopa|           13|\n",
            "|2021-01-01| Arizona|              Navajo|           14|\n",
            "|2021-01-01| Arizona|                Pima|           15|\n",
            "|2021-01-01|Arkansas|             Pulaski|           10|\n",
            "+----------+--------+--------------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not modify\n",
        "print('Rows in county_max_temp_hour:', county_max_temp_hour.count())\n",
        "county_max_temp_hour.orderBy('date', 'state', 'county').show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kFdZN_4IeutU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c982ef-1024-4f91-dc53-ec479333d5a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows in temperature_county_final: 138555\n",
            "+----------+--------+--------------------+------------------+----------+---------------+-------------+\n",
            "|      date|   state|              county|mean_temperature_f|max_temp_f|sites_reporting|max_temp_hour|\n",
            "+----------+--------+--------------------+------------------+----------+---------------+-------------+\n",
            "|2021-01-01| Alabama|            Escambia|                64|        69|              1|            0|\n",
            "|2021-01-01| Alabama|           Jefferson|                66|        74|              1|           13|\n",
            "|2021-01-01|  Alaska|              Denali|                -1|         6|              1|            4|\n",
            "|2021-01-01|  Alaska|Fairbanks North Star|                -9|        -1|              2|           12|\n",
            "|2021-01-01| Arizona|             Cochise|                40|        49|              1|           12|\n",
            "|2021-01-01| Arizona|            Coconino|                29|        35|              1|           12|\n",
            "|2021-01-01| Arizona|            Maricopa|                49|        65|              1|           13|\n",
            "|2021-01-01| Arizona|              Navajo|                30|        42|              1|           14|\n",
            "|2021-01-01| Arizona|                Pima|                47|        63|              4|           15|\n",
            "|2021-01-01|Arkansas|             Pulaski|                42|        45|              1|           10|\n",
            "+----------+--------+--------------------+------------------+----------+---------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not modify\n",
        "print('Rows in temperature_county_final:', temperature_county_final.count())\n",
        "temperature_county_final.orderBy('date', 'state', 'county').show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t7rZEs2fDvh"
      },
      "source": [
        "### Q9\n",
        "\n",
        "Join `aqi` to `temperature_county_final` and call the resulting data frame `daily_county_measurements`. Then write code that produces the `date`, name of the `county`, `state`, and `aqi` value where the **highest recorded `aqi`** occurred in 2021. In the event of a tie, take the first instance. Your answer should take the following form:\n",
        "\n",
        "\"The highest recorded AQI value in 2021 occured on [date], in [county] County, [state], and had a value of [aqi].\"\n",
        "\n",
        "I have included some comments and starter code to help you out."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join aqi to temperature_county_final and create daily_county_measurements\n",
        "daily_county_measurements = temperature_county_final.join(aqi, [\"date\", \"state\", \"county\"], \"inner\")\n",
        "\n",
        "# Find the maximum AQI value in 2021\n",
        "max_aqi = daily_county_measurements.filter(daily_county_measurements[\"date\"].like(\"2021%\")) \\\n",
        "    .agg(max(\"aqi\").alias(\"max_aqi\"))\n",
        "\n",
        "# Create a DataFrame with a single row that contains the highest recorded AQI value in 2021\n",
        "highest_aqi = daily_county_measurements.join(max_aqi,\n",
        "                                           [max_aqi[\"max_aqi\"] == daily_county_measurements[\"aqi\"]],\n",
        "                                           \"inner\").limit(1)\n",
        "\n",
        "# Extract values from the data frame\n",
        "row = highest_aqi.collect()[0]\n",
        "date = row['date']\n",
        "county = row['county']\n",
        "state = row['state']\n",
        "aqi_value = row['max_aqi']\n",
        "\n",
        "# Print the output as specified\n",
        "print(f\"The highest recorded AQI value in 2021 occurred on {date}, in {county} County, {state}, and had a value of {aqi_value}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y_F0_3uHfRq",
        "outputId": "f11b823d-4c30-4612-e76b-86ba3f10e998"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The highest recorded AQI value in 2021 occurred on 2021-09-14, in Tulare County, California, and had a value of 537.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoQjhhS3LpbR"
      },
      "source": [
        "### Q10\n",
        "\n",
        "Using a process similar to Q8, create a new data frame called `highest_temperates_by_state_2021` that contains one row per state, and shows the `date`, `state`, and `max_temp_f` for the **highest recorded temperature** in that state in 2021. In the case of ties, pick the earliest day of the year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "g_i-bD5PzMV8"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "# Define a window spec\n",
        "window_spec = Window.partitionBy(\"state\", \"date\")\\\n",
        "  .orderBy(desc(\"max_temp_f\"))\n",
        "\n",
        "# Add row number for each group\n",
        "df = temperature.filter(temperature[\"date\"].like(\"2021%\"))\\\n",
        "  .withColumn('row_num', row_number().over(window_spec))\n",
        "\n",
        "# Filter to the first row using row_num\n",
        "df2 = df.where(df[\"row_num\"] == 1)\n",
        "\n",
        "# Keep only the grouping columns and max_temp_f\n",
        "highest_temperates_by_state_2021 = df2.select(\"date\", \"state\", \"max_temp_f\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EY9gDNeb2LfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81221d5d-14c5-45a0-c1a7-9166c180708d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+----------+\n",
            "|      date|            state|max_temp_f|\n",
            "+----------+-----------------+----------+\n",
            "|2021-11-20|       California|     129.0|\n",
            "|2021-07-10|       California|     124.0|\n",
            "|2021-06-17|Country Of Mexico|     123.1|\n",
            "|2021-07-11|       California|     123.0|\n",
            "|2021-06-18|Country Of Mexico|     122.2|\n",
            "|2021-07-09|       California|     121.0|\n",
            "|2021-07-12|       California|     121.0|\n",
            "|2021-08-04|Country Of Mexico|     120.9|\n",
            "|2021-06-19|Country Of Mexico|     120.2|\n",
            "|2021-06-17|       California|     120.0|\n",
            "|2021-07-08|       California|     120.0|\n",
            "|2021-08-04|       California|     120.0|\n",
            "|2021-08-27|Country Of Mexico|     119.7|\n",
            "|2021-06-18|       California|     119.3|\n",
            "|2021-06-15|Country Of Mexico|     119.3|\n",
            "|2021-08-03|Country Of Mexico|     119.1|\n",
            "|2021-06-16|       California|     119.0|\n",
            "|2021-06-19|       California|     119.0|\n",
            "|2021-07-07|       California|     119.0|\n",
            "|2021-06-17|          Arizona|     118.0|\n",
            "+----------+-----------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not modify\n",
        "highest_temperates_by_state_2021\\\n",
        "  .select('date', 'state', 'max_temp_f')\\\n",
        "  .orderBy(desc('max_temp_f'))\\\n",
        "  .show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}